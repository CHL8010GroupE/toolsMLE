---
title: "Normal(Gaussian)"
format: html
editor: visual
---

# Algorithm for MLE in Normal (Gaussian) Distribution

To implement the Maximum Likelihood Estimation (MLE) for the Gaussian distribution, we can derive the estimates of the parameters (mean μ and standard deviation σ) from the given data.

### Step 1: Define the Likelihood Function

For a dataset `X = (x1, x2, ..., xn)` sampled from a Gaussian distribution, the likelihood function for the parameters μ and σ is given by:

$$ L(μ, σ; X) = \prod_{i=1}^{n} \frac{1}{σ \sqrt{2\pi}} \exp\left( -\frac{(x_i - μ)^2}{2σ^2} \right) $$

### Step 2: Define the Log-Likelihood Function

It is often easier to work with the log-likelihood instead of the likelihood itself due to computational reasons (products become sums). The log-likelihood function is:

$$ \ell(μ, σ; X) = -n \log(σ \sqrt{2\pi}) - \frac{1}{2σ^2} \sum_{i=1}^{n} (x_i - μ)^2 $$

### Step 3: Derive MLE Equations

To find the estimates of μ and σ, take partial derivatives of the log-likelihood function with respect to μ and σ, and set them to zero.

**Partial Derivative with respect to μ:**

$$ \frac{\partial \ell}{\partial μ} = \frac{1}{σ^2} \sum_{i=1}^{n} (x_i - μ) = 0  $$

Solving for μ, we get:

$$\hat{μ} = \frac{1}{n} \sum_{i=1}^{n} x_i  $$

This means that the MLE for μ is the sample mean.

**Partial Derivative with respect to σ:**

$$ \frac{\partial \ell}{\partial σ} = -\frac{n}{σ} + \frac{1}{σ^3} \sum_{i=1}^{n} (x_i - μ)^2 = 0 $$

Solving for σ, we get:

$$\hat{σ} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - μ)^2} $$

This means that the MLE for σ is the square root of the average squared deviation from the mean.

### Step 4: Implementing the MLE in R

Here is a function to compute the MLE for the Gaussian distribution in R:

```{r}
mle_gaussian <- function(x) {
  n <- length(x)
  mu_hat <- mean(x)
  sigma_hat <- sqrt(sum((x - mu_hat)^2) / n)
  
  return(list(mean = mu_hat, sigma = sigma_hat))
}
```

### Step 5: Likelihood Ratio Function Explained

The likelihood ratio tests compare two different models: one that is more general (allowing more variability in parameters) and one that is more restricted. To compute the likelihood ratio for our Gaussian model, we need to compare how well different values of $\hat\mu$ and $\hat\sigma^2$ fit the data compared to the estimated values ($\hat\mu$ and $\hat\sigma^2$).

Here is a step-by-step implementation:

1.  **Define the Likelihood Function**: The likelihood function has already been defined as `normll`, which calculates the log-likelihood of a normal distribution given $\hat\mu$, $\hat\sigma^2$, and data x.

2.  **Likelihood Ratio Calculation**: The likelihood ratio is calculated as the difference between the log-likelihood of given parameters ($\hat\mu$, $\hat\sigma^2$) and the log-likelihood at the MLE estimates ($\hat\mu$, $\hat\sigma^2$). The likelihood ratio helps in comparing how well different parameter values fit compared to the best-fit values.
